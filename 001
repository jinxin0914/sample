import logging
from datetime import datetime
import psycopg2
from smb.SMBConnection import SMBConnection
import tempfile
import os

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sync_log.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# SMB connection configuration
SMB_CONFIG = {
    'username': 'your_username',  # Replace with your Windows share folder username
    'password': 'your_password',  # Replace with your password
    'client_name': 'databricks',
    'server_name': 'arxhz2',
    'server_ip': 'arxhz2_ip',  # Replace with arxhz2 server IP address
    'domain': 'your_domain',  # Replace with your domain name, if applicable
    'share_name': 'Files',
    'share_path': 'ERMRiskMO/PROD/SFP'
}

# S3 configuration (Databricks External Volume)
S3_VOLUME_PATH = '/Volumes/your_catalog/your_volume'  # Replace with your Databricks External Volume path
S3_PREFIX = 'model_outputs/'

# Database configuration
DB_CONFIG = {
    'dbname': 'your_db_name',
    'user': 'your_username',
    'password': 'your_password',
    'host': 'your_host',
    'port': '5432'
}

# User information
CREATED_BY = 'sync_script'
UPDATED_BY = 'sync_script'

def connect_to_smb():
    """Connect to Windows share folder"""
    try:
        conn = SMBConnection(
            SMB_CONFIG['username'],
            SMB_CONFIG['password'],
            SMB_CONFIG['client_name'],
            SMB_CONFIG['server_name'],
            domain=SMB_CONFIG['domain'],
            use_ntlm_v2=True
        )
        
        connected = conn.connect(SMB_CONFIG['server_ip'], 139)  # SMB default port
        
        if not connected:
            logger.error("Unable to connect to SMB server")
            return None
            
        logger.info("Successfully connected to SMB server")
        return conn
    except Exception as e:
        logger.error(f"Error connecting to SMB server: {e}")
        return None

def get_processed_files():
    """Get list of processed files from database"""
    processed_files = {}
    
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        # Get non-deleted file records
        cursor.execute("""
            SELECT run_id, file_path, file_name 
            FROM model_output 
            WHERE is_deleted = FALSE
        """)
        
        for run_id, file_path, file_name in cursor.fetchall():
            key = f"{run_id}_{file_name}"
            processed_files[key] = file_path
            
        cursor.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error getting processed file list: {e}")
    
    return processed_files

def sync_to_s3():
    """Synchronize files to S3 and update database"""
    try:
        # Connect to SMB server
        smb_conn = connect_to_smb()
        if not smb_conn:
            return
        
        # Get processed files
        processed_files = get_processed_files()
        
        # Connect to database
        db_conn = psycopg2.connect(**DB_CONFIG)
        cursor = db_conn.cursor()
        
        # Get directory list from share folder
        share_dirs = smb_conn.listPath(SMB_CONFIG['share_name'], f"/{SMB_CONFIG['share_path']}/")
        
        # Scan source directory
        for item in share_dirs:
            # Skip . and .. directories and non-directory items
            if item.filename in ['.', '..'] or not item.isDirectory:
                continue
                
            # Check if directory name is a number (run_id)
            if item.filename.isdigit():
                run_id = int(item.filename)
                output_path = f"/{SMB_CONFIG['share_path']}/{item.filename}/Output"
                
                # Check if Output directory exists
                try:
                    output_files = smb_conn.listPath(SMB_CONFIG['share_name'], output_path)
                    
                    # Process files in Output directory
                    for file_item in output_files:
                        # Skip . and .. and directories
                        if file_item.filename in ['.', '..'] or file_item.isDirectory:
                            continue
                            
                        file_name = file_item.filename
                        smb_file_path = f"{output_path}/{file_name}"
                        key = f"{run_id}_{file_name}"
                        
                        # Build S3 path
                        s3_file_path = f"{S3_PREFIX}{run_id}/Output/{file_name}"
                        s3_full_path = f"{S3_VOLUME_PATH}/{s3_file_path}"
                        
                        # Write SMB file content directly to temporary file, optimized for large files
                        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                            temp_path = temp_file.name
                            smb_conn.retrieveFile(SMB_CONFIG['share_name'], smb_file_path, temp_file)
                        
                        try:
                            # Import Databricks utilities
                            from pyspark.dbutils import DBUtils
                            dbutils = DBUtils(spark)
                            
                            # Ensure target directory exists
                            target_dir = os.path.dirname(s3_full_path)
                            dbutils.fs.mkdirs(target_dir)
                            
                            # Upload file to S3
                            dbutils.fs.cp(f"file://{temp_path}", s3_full_path)
                            logger.info(f"Uploaded file to S3: {smb_file_path} -> {s3_full_path}")
                            
                            # Update or insert database record
                            if key in processed_files:
                                # Update existing record
                                cursor.execute("""
                                    UPDATE model_output 
                                    SET updated_at = %s, updated_by = %s
                                    WHERE run_id = %s AND file_name = %s AND is_deleted = FALSE
                                """, (datetime.now(), UPDATED_BY, run_id, file_name))
                            else:
                                # Insert new record
                                cursor.execute("""
                                    INSERT INTO model_output 
                                    (run_id, file_path, file_name, created_by, updated_by)
                                    VALUES (%s, %s, %s, %s, %s)
                                """, (run_id, s3_file_path, file_name, CREATED_BY, UPDATED_BY))
                            
                            logger.info(f"Updated database record: run_id={run_id}, file_name={file_name}")
                        finally:
                            # Delete temporary file
                            os.unlink(temp_path)
                except Exception as e:
                    logger.error(f"Error processing directory {item.filename}: {e}")
        
        # Commit transaction and close connections
        db_conn.commit()
        cursor.close()
        db_conn.close()
        smb_conn.close()
        
        logger.info("Synchronization completed")
        
    except Exception as e:
        logger.error(f"Error during synchronization: {e}")
        if 'db_conn' in locals() and db_conn is not None:
            db_conn.rollback()
            db_conn.close()
        if 'smb_conn' in locals() and smb_conn is not None:
            smb_conn.close()

if __name__ == "__main__":
    logger.info("Starting synchronization process")
    sync_to_s3()
